#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆè¯„ä¼°è„šæœ¬ï¼šå¯¹æ¯”éšæœºåˆå§‹åŒ–æ¨¡å‹å’Œ LoRA å¾®è°ƒåçš„æ¨¡å‹
ä¸éœ€è¦ä¸‹è½½å¤–éƒ¨é¢„è®­ç»ƒæ¨¡å‹
"""

import os
import sys
sys.path.insert(0, '/root/bioclip-2/src')

import torch
import pandas as pd
from PIL import Image
from tqdm import tqdm
import numpy as np
from collections import defaultdict

from open_clip import create_model_and_transforms, get_tokenizer
from open_clip.lora import apply_lora_to_model, load_lora_weights

# é…ç½®
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
MODEL_NAME = "ViT-L-14"
LORA_WEIGHTS_PATH = "/root/bioclip-2/logs/2025_12_16-11_40_22-model_ViT-L-14-lr_0.0005-b_32-j_4-p_bf16/checkpoints/lora_epoch_20.pt"
VAL_DATA_PATH = "/root/bioclip-2/data/val_data.csv"

# LoRA é…ç½®ï¼ˆéœ€è¦ä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
LORA_RANK = 8
LORA_ALPHA = 16.0


def load_lora_model():
    """åŠ è½½å¸¦æœ‰ LoRA æƒé‡çš„æ¨¡å‹"""
    print("åŠ è½½ LoRA å¾®è°ƒåçš„æ¨¡å‹...")
    model, _, preprocess = create_model_and_transforms(
        MODEL_NAME,
        pretrained=None  # ä¸è®­ç»ƒæ—¶ä¸€è‡´
    )
    
    # åº”ç”¨ LoRA
    model, _ = apply_lora_to_model(
        model, 
        rank=LORA_RANK, 
        alpha=LORA_ALPHA,
        enable_vision=True,
        enable_text=True
    )
    
    # åŠ è½½ LoRA æƒé‡
    load_lora_weights(model, LORA_WEIGHTS_PATH)
    model = model.to(DEVICE).eval()
    return model, preprocess


def load_random_baseline():
    """åŠ è½½éšæœºåˆå§‹åŒ–æ¨¡å‹ï¼ˆè®­ç»ƒèµ·ç‚¹å‚è€ƒï¼‰"""
    print("åŠ è½½éšæœºåˆå§‹åŒ–æ¨¡å‹ (è®­ç»ƒèµ·ç‚¹å‚è€ƒ)...")
    model, _, preprocess = create_model_and_transforms(
        MODEL_NAME,
        pretrained=None
    )
    model = model.to(DEVICE).eval()
    return model, preprocess


def get_class_names(df):
    """è·å–ç±»åˆ«åç§°åˆ—è¡¨"""
    return sorted(df['caption'].unique().tolist())


def create_text_features(model, tokenizer, class_names):
    """ä¸ºæ‰€æœ‰ç±»åˆ«åˆ›å»ºæ–‡æœ¬ç‰¹å¾"""
    templates = [
        "a photo of {}",
        "an image of {}",
        "{}"
    ]
    
    text_features_list = []
    
    with torch.no_grad():
        for class_name in class_names:
            class_features = []
            for template in templates:
                text = template.format(class_name)
                tokens = tokenizer([text]).to(DEVICE)
                features = model.encode_text(tokens)
                # å¤„ç†å¯èƒ½è¿”å› tuple çš„æƒ…å†µ
                if isinstance(features, tuple):
                    features = features[0]
                features = features / features.norm(dim=-1, keepdim=True)
                class_features.append(features)
            
            avg_features = torch.stack(class_features).mean(dim=0)
            avg_features = avg_features / avg_features.norm(dim=-1, keepdim=True)
            text_features_list.append(avg_features)
    
    text_features = torch.cat(text_features_list, dim=0)
    return text_features


def evaluate_model(model, preprocess, tokenizer, df, class_names, model_name="Model"):
    """è¯„ä¼°æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šçš„æ€§èƒ½"""
    print(f"\nè¯„ä¼° {model_name}...")
    
    text_features = create_text_features(model, tokenizer, class_names)
    
    correct_top1 = 0
    correct_top5 = 0
    total = 0
    
    class_correct = defaultdict(int)
    class_total = defaultdict(int)
    
    with torch.no_grad():
        for idx, row in tqdm(df.iterrows(), total=len(df), desc=f"è¯„ä¼° {model_name}"):
            filepath = row['filepath']
            true_label = str(row['caption'])
            
            try:
                image = Image.open(filepath).convert('RGB')
                image_input = preprocess(image).unsqueeze(0).to(DEVICE)
                
                image_features = model.encode_image(image_input)
                # å¤„ç†å¯èƒ½è¿”å› tuple çš„æƒ…å†µ
                if isinstance(image_features, tuple):
                    image_features = image_features[0]
                image_features = image_features / image_features.norm(dim=-1, keepdim=True)
                
                similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)
                
                values, indices = similarity[0].topk(5)
                top1_pred = class_names[indices[0].item()]
                top5_preds = [class_names[i.item()] for i in indices]
                
                if top1_pred == true_label:
                    correct_top1 += 1
                    class_correct[true_label] += 1
                
                if true_label in top5_preds:
                    correct_top5 += 1
                
                class_total[true_label] += 1
                total += 1
                
            except Exception as e:
                print(f"å¤„ç† {filepath} æ—¶å‡ºé”™: {e}")
                continue
    
    top1_acc = correct_top1 / total * 100 if total > 0 else 0
    top5_acc = correct_top5 / total * 100 if total > 0 else 0
    
    print(f"\n{model_name} ç»“æœ:")
    print(f"  Top-1 å‡†ç¡®ç‡: {top1_acc:.2f}%")
    print(f"  Top-5 å‡†ç¡®ç‡: {top5_acc:.2f}%")
    print(f"  æ€»æ ·æœ¬æ•°: {total}")
    
    print(f"\n  å„ç±»åˆ« Top-1 å‡†ç¡®ç‡:")
    for class_name in class_names:
        if class_total[class_name] > 0:
            acc = class_correct[class_name] / class_total[class_name] * 100
            print(f"    {class_name}: {acc:.2f}% ({class_correct[class_name]}/{class_total[class_name]})")
    
    return {
        'top1_acc': top1_acc,
        'top5_acc': top5_acc,
        'total': total,
        'class_acc': {c: class_correct[c]/class_total[c]*100 if class_total[c] > 0 else 0 for c in class_names}
    }


def main():
    print("="*60)
    print("LoRA å¾®è°ƒæ•ˆæœéªŒè¯è¯„ä¼°")
    print("="*60)
    
    print(f"\nåŠ è½½éªŒè¯æ•°æ®: {VAL_DATA_PATH}")
    df = pd.read_csv(VAL_DATA_PATH, sep='\t')
    print(f"éªŒè¯é›†æ ·æœ¬æ•°: {len(df)}")
    
    class_names = get_class_names(df)
    print(f"ç±»åˆ«æ•°: {len(class_names)}")
    print(f"ç±»åˆ«: {class_names}")
    
    tokenizer = get_tokenizer(MODEL_NAME)
    
    results = {}
    
    # 1. è¯„ä¼°éšæœºåˆå§‹åŒ–æ¨¡å‹ï¼ˆè®­ç»ƒèµ·ç‚¹ï¼‰
    print("\n" + "-"*40)
    random_model, preprocess = load_random_baseline()
    results['random_init'] = evaluate_model(
        random_model, preprocess, tokenizer, df, class_names,
        model_name="éšæœºåˆå§‹åŒ–æ¨¡å‹ (è®­ç»ƒèµ·ç‚¹)"
    )
    del random_model
    torch.cuda.empty_cache()
    
    # 2. è¯„ä¼° LoRA å¾®è°ƒåçš„æ¨¡å‹
    print("\n" + "-"*40)
    lora_model, preprocess = load_lora_model()
    results['lora'] = evaluate_model(
        lora_model, preprocess, tokenizer, df, class_names,
        model_name="LoRA å¾®è°ƒæ¨¡å‹ (20 epochs)"
    )
    
    # å¯¹æ¯”ç»“æœ
    print("\n" + "="*60)
    print("ğŸ“Š å¯¹æ¯”ç»“æœæ€»ç»“")
    print("="*60)
    
    print(f"\n{'æ¨¡å‹':<35} {'Top-1 å‡†ç¡®ç‡':<15} {'Top-5 å‡†ç¡®ç‡':<15}")
    print("-"*65)
    print(f"{'éšæœºåˆå§‹åŒ– (è®­ç»ƒèµ·ç‚¹)':<35} {results['random_init']['top1_acc']:<15.2f} {results['random_init']['top5_acc']:<15.2f}")
    print(f"{'LoRAå¾®è°ƒå (20 epochs)':<35} {results['lora']['top1_acc']:<15.2f} {results['lora']['top5_acc']:<15.2f}")
    
    # è®¡ç®—æå‡
    improvement_top1 = results['lora']['top1_acc'] - results['random_init']['top1_acc']
    improvement_top5 = results['lora']['top5_acc'] - results['random_init']['top5_acc']
    
    print(f"\nğŸ“ˆ LoRA å¾®è°ƒæ•ˆæœ:")
    print(f"   Top-1 æå‡: {'+' if improvement_top1 >= 0 else ''}{improvement_top1:.2f}%")
    print(f"   Top-5 æå‡: {'+' if improvement_top5 >= 0 else ''}{improvement_top5:.2f}%")
    
    # å„ç±»åˆ«å¯¹æ¯”
    print(f"\nğŸ“‹ å„ç±»åˆ«å¯¹æ¯”:")
    for class_name in class_names:
        random_acc = results['random_init']['class_acc'][class_name]
        lora_acc = results['lora']['class_acc'][class_name]
        diff = lora_acc - random_acc
        print(f"   {class_name}: {random_acc:.1f}% â†’ {lora_acc:.1f}% ({'+' if diff >= 0 else ''}{diff:.1f}%)")
    
    # ä¿å­˜ç»“æœ
    results_path = "/root/bioclip-2/logs/evaluation_results.txt"
    with open(results_path, 'w') as f:
        f.write("LoRA å¾®è°ƒæ•ˆæœéªŒè¯è¯„ä¼°ç»“æœ\n")
        f.write("="*60 + "\n\n")
        f.write(f"éªŒè¯é›†æ ·æœ¬æ•°: {len(df)}\n")
        f.write(f"ç±»åˆ«æ•°: {len(class_names)}\n")
        f.write(f"ç±»åˆ«: {class_names}\n\n")
        
        f.write(f"1. éšæœºåˆå§‹åŒ–æ¨¡å‹ (è®­ç»ƒèµ·ç‚¹):\n")
        f.write(f"   Top-1 å‡†ç¡®ç‡: {results['random_init']['top1_acc']:.2f}%\n")
        f.write(f"   Top-5 å‡†ç¡®ç‡: {results['random_init']['top5_acc']:.2f}%\n\n")
        
        f.write(f"2. LoRA å¾®è°ƒæ¨¡å‹ (20 epochs):\n")
        f.write(f"   Top-1 å‡†ç¡®ç‡: {results['lora']['top1_acc']:.2f}%\n")
        f.write(f"   Top-5 å‡†ç¡®ç‡: {results['lora']['top5_acc']:.2f}%\n\n")
        
        f.write(f"ğŸ“ˆ æå‡æ•ˆæœ:\n")
        f.write(f"   Top-1: {'+' if improvement_top1 >= 0 else ''}{improvement_top1:.2f}%\n")
        f.write(f"   Top-5: {'+' if improvement_top5 >= 0 else ''}{improvement_top5:.2f}%\n\n")
        
        f.write(f"å„ç±»åˆ«å¯¹æ¯”:\n")
        for class_name in class_names:
            random_acc = results['random_init']['class_acc'][class_name]
            lora_acc = results['lora']['class_acc'][class_name]
            diff = lora_acc - random_acc
            f.write(f"   {class_name}: {random_acc:.1f}% â†’ {lora_acc:.1f}% ({'+' if diff >= 0 else ''}{diff:.1f}%)\n")
    
    print(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ°: {results_path}")
    
    return results


if __name__ == "__main__":
    main()

